#!/usr/bin/env python
# coding: utf-8

# In[13]:


get_ipython().run_cell_magic(u'js', u'', u'alert("WELCOME TO MACHINE LEARNING")')


# <h4>Tasks:-</h4>
# <ol>
#     <li>loading dataset</li>
#     <li>resampling</li>
#     <li>train test split</li>
#     <li>Estimator</li>
#     <li>cross validation</li>
#     <li>GridSearchCV</li>
#     <li>RandomizedSearchCV</li>
#     <li>final predicting the result</li>
#     <li>accuracy</li>
#     <li>confusion matrix</li>
#     <li>modifing the threshold</li>
#     <li>ROC</li>
#     <li>AUC,area under curve</li>
# </ol>

# <h2>1. Loading dataset</h2>

# In[14]:


from sklearn.datasets import load_iris
raw_data = load_iris()
X = raw_data.data
Y = raw_data.target
#print raw_data.DESCR
detail = raw_data.data[0:1]
print (detail)


# In[15]:


#printing the distribution of samples in each class
from collections import Counter

print sorted(Counter(Y).items())


# <h2>2. Resampling</h2>

# In[16]:


#if the samples were not distributed among classes then we would have to do resampling as follow
from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state = 2)
X_resampled ,Y_resampled = ros.fit_resample(X,Y)

print sorted(Counter(Y).items())


# <h2>3. train_test_split</h2>

# In[17]:


from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test = train_test_split(X,Y)


# <h2>4. Estimator</h2>

# In[18]:


from sklearn.neighbors import KNeighborsClassifier
knc = KNeighborsClassifier(n_neighbors = 10)
knc.fit(X_train,Y_train)


# In[19]:


y_pred = knc.predict(X_test)


# In[20]:


from sklearn.metrics import accuracy_score
print accuracy_score(Y_test,y_pred)


# <h2>5. Cross Validation</h2>

# In[21]:


# first we will divide the dataset into 10 different folds, where each fold is consists of training and testing set
from sklearn.model_selection import KFold
kf = KFold(n_splits = 10) #here n_splits = 10, means 10 folds
# we can print no. of flods as follow
print kf.get_n_splits


# In[22]:


# Now we can print all the 10 folds each containg training and testing sets

for train_index,test_index in kf.split(X): # split() method returns all indices
    print 'training set: ',train_index
    print 'testing set : ',test_index
    print '\n'


# In[23]:


from sklearn.model_selection import cross_val_score
cvs = cross_val_score(knc, X, Y, cv=10, scoring ='accuracy')# NOTE: Here knc has parameter n_neighbors=10,fixed
print cvs  # cvs object contains the accuracy in each fold


# In[24]:


# printing the mean accuracy of each fold
print cvs.mean()


# In[25]:


# we can vary the value of n_neighbors in a given range
x_range = range(1,20)
y_range = []
for n in x_range:
    knc = KNeighborsClassifier(n_neighbors = n)
    cvs = cross_val_score(knc, X, Y, cv=10, scoring = 'accuracy')
    y_range.append(cvs.mean())
print y_range


# In[26]:


print sorted(Counter(y_range).items())


# In[27]:


import numpy as np
np.mean(y_range)


# <h2>6. GridSearchCV</h2>

# In[28]:


# First we'll make grid to figure out hyperparameter. For this we'll make a grid(dictionary) .
# This is said to be Tuning of parameter.
param_grid = dict(n_neighbors = x_range)
print param_grid


# In[29]:


from sklearn.model_selection import GridSearchCV
gscv =  GridSearchCV(knc, param_grid, cv=10, scoring='accuracy') # gscv object contains the knc with hyperparameter.


# In[30]:


print gscv


# In[31]:


gscv.fit(X,Y)


# In[32]:


print gscv.cv_results_


# In[33]:


print gscv.best_params_


# In[34]:


print gscv.best_score_


# In[35]:


# Now we'll tune multiple parameters by make same parameter grid
weights = ['uniform','distance']
param_grid = dict(n_neighbors=x_range,weight=weights)


# In[36]:


print param_grid


# In[37]:


gscv =  GridSearchCV(knc, param_grid, cv=10, scoring='accuracy')


# In[38]:


gscv.get_params





